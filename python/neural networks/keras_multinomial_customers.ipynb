{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction  \n",
    "\n",
    "This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Import Classes and Functions\n",
    "\n",
    "We can begin by importing all of the classes and functions we will need in this tutorial.\n",
    "\n",
    "This includes both the functionality we require from Keras, but also data loading from pandas as well as data preparation and model evaluation from scikit-learn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load\n",
    "import pandas as pd \n",
    "import numpy as np "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Train and save model  \n",
    "\n",
    "## 3.1 Load our data  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/customertrain.csv')\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that we have 8068 training examples, but we do have some things to sort out:  \n",
    "\n",
    "- We will neeed to deal with all the null values in some of the features and we will auto generate values\n",
    "- Our output variables 'Y' also has nulls, we will remove those rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepareY(df):\n",
    "    # Drop rows with no Output values\n",
    "    df.dropna(subset=['Var_1'], inplace=True)\n",
    "\n",
    "    # extract Y and drop from dataframe\n",
    "    Y = df[\"Var_1\"]\n",
    "\n",
    "    # encode class values as integers\n",
    "    yencoder = LabelEncoder()\n",
    "    yencoder.fit(Y)\n",
    "    dump(yencoder,\"models/yencoder.joblib\")\n",
    "    # np.save('models/y_classes.npy', yencoder.classes_) # save for later\n",
    "    return yencoder.transform(Y)\n",
    "    # encoded_Y = yencoder.transform(Y)\n",
    "\n",
    "    # # convert integers to one hot encoded)\n",
    "    # return np_utils.to_categorical(encoded_Y), encoded_Y\n",
    "\n",
    "\n",
    "y = prepareY(df)\n",
    "df = df.drop([\"Var_1\"], axis=1)\n",
    "pd.DataFrame(y).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Prepare our features  \n",
    "\n",
    "## fillmissing  \n",
    "\n",
    "An important part of regression is understanding which features are missing. We can choose to ignore all rows with missing values, or fill them in with either mode, median or mode.  \n",
    "\n",
    "- Mode = most common value\n",
    "- Median = middle value\n",
    "- Mean = average\n",
    "\n",
    "This is a handy function you can call which will fill in the missing features by your desired method. We will choose to fill in values with the average.  \n",
    "\n",
    "## prepareFeatures  \n",
    "\n",
    "We need to do a few things to our features, so we can work with them a little easier.  \n",
    "\n",
    "- Lets convert our string fields to numbers\n",
    "- Call fillmissing to fill in any missing values in our features.  \n",
    "- Use MinMaxScaler to normalise our numbers so thay have mean of zero with a deviation of 1.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fillmissing(df, feature, method):\n",
    "    \"\"\"Fills in missing values in features so we do not loose the rows\n",
    "\n",
    "      Parameters:\n",
    "      df (DataFrame): The dataframe with features\n",
    "      feature (string): The feature to fill in\n",
    "      method (string): replace the nill with either mode, median or default to mean\n",
    "    \"\"\"  \n",
    "    if method == \"mode\":\n",
    "      df[feature] = df[feature].fillna(df[feature].mode()[0])\n",
    "    elif method == \"median\":\n",
    "      df[feature] = df[feature].fillna(df[feature].median())\n",
    "    else:\n",
    "      df[feature] = df[feature].fillna(df[feature].mean())\n",
    "\n",
    "def prepareFeatures(df):\n",
    "    \"\"\"Prepares feature for ML\n",
    "\n",
    "      Parameters:\n",
    "      df (DataFrame): The dataframe with features\n",
    "\n",
    "      Returns:\n",
    "      X (nparray): a normalised array of all features\n",
    "    \"\"\"  \n",
    "    # Encode string features to numerics\n",
    "    columns = df.select_dtypes(include=['object']).columns\n",
    "    # columns = [\"Gender\",\"Ever_Married\",\"Graduated\",\"Profession\",\"Spending_Score\"]\n",
    "    for feature in columns:\n",
    "        le = LabelEncoder()\n",
    "        df[feature] = le.fit_transform(df[feature])\n",
    "        # np.save('models/'+feature+'_classes.npy', le.classes_) # save for later\n",
    "        dump(le, 'models/'+feature+'.joblib') \n",
    "\n",
    "    # fill in missing features with mean values\n",
    "    features_missing = df.columns[df.isna().any()]\n",
    "    for feature in features_missing:\n",
    "      fillmissing(df, feature= feature, method= \"mean\")    \n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X = scaler.fit_transform(df)\n",
    "    dump(scaler, \"models/featurescaler.joblib\") \n",
    "    # np.save('models/featurescaler.npy', le.classes_) # save for later\n",
    "    return X, df\n",
    "\n",
    "df = df.drop([\"Segmentation\",\"ID\"], axis=1) # These fields not features we can use\n",
    "X, df = prepareFeatures(df)\n",
    "pd.DataFrame(X).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After funning below, you should see 7992 with non-null values and all should be float64."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame(X).info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.3 Split train and test data  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.4 Hot Encoding Y\n",
    "\n",
    "The output variable contains six different string values.\n",
    "\n",
    "When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "\n",
    "This is called `one hot encoding` or creating dummy variables from a categorical variable.\n",
    "\n",
    "For example, in this problem six class values are [1,2,3,4,5,6]. We can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:    \n",
    "  \n",
    "![onehot](./images/y_one_hot.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yhot = np_utils.to_categorical(y)\n",
    "yhot_train = np_utils.to_categorical(y_train)\n",
    "yhot_test = np_utils.to_categorical(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.5 Define The Neural Network Model\n",
    "\n",
    "There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.\n",
    "\n",
    "Below is a function that will create a baseline neural network for the customer classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.\n",
    "\n",
    "The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our customer dataset, the output layer must create 6 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.\n",
    "\n",
    "So, now you are asking “What are reasonable numbers to set these to?”  \n",
    "\n",
    "- Input layer = set to the size of the features, but add a bias neuron (ie. 9)\n",
    "- Hidden layers = set to input_layer * 2 (ie. 18)\n",
    "- Output layer = set to the size of the labels of Y. In our case, this is 7 categories\n",
    "\n",
    "The network topology of this simple one-layer neural network can be summarized as:\n",
    "\n",
    "```\n",
    "9 inputs -> [18 hidden nodes] -> 7 outputs\n",
    "```\n",
    "\n",
    "Note that we use a **softmax** activation function in the output layer. This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.\n",
    "\n",
    "Finally, the network uses the efficient **Adam gradient descent optimization algorithm** with a logarithmic loss function, which is called **categorical_crossentropy** in Keras."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\t# Rectified Linear Unit Activation Function\n",
    "\tmodel.add(Dense(16, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(16, activation = 'relu'))\n",
    "\t# Softmax for multi-class classification\n",
    "\tmodel.add(Dense(7, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now create our KerasClassifier for use in scikit-learn.\n",
    "\n",
    "We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.  \n",
    "\n",
    "Advantages of using a batch size < number of all samples:  \n",
    "\n",
    "- It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory.  \n",
    "- Typically networks train faster with mini-batches. That's because we update the weights after each propagation.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model = baseline_model()\n",
    "cmodel = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=100, verbose=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.6 Evaluate The Model with k-Fold Cross Validation\n",
    "\n",
    "Now, lets evaluate the neural network model on our training data.\n",
    "\n",
    "The scikit-learn evaluates models using various techniques. The gold standard for evaluating machine learning models is **k-fold cross validation**.\n",
    "\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.  \n",
    "2. Split the dataset into k groups  \n",
    "3. For each unique group:  \n",
    "3.1 Take the group as a hold out or test data set  \n",
    "3.2 Take the remaining groups as a training data set  \n",
    "3.3 Fit a model on the training set and evaluate it on the test set  \n",
    "3.4 Retain the evaluation score and discard the model  \n",
    "4. Summarize the skill of the model using the sample of model evaluation scores  \n",
    "\n",
    "Lets define the model evaluation procedure. Here, we set  \n",
    "\n",
    "- The number of folds to be 10 (a good default) \n",
    "- Shuffle the data before partitioning it. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can evaluate our model (estimator) on our dataset (X and hot_y) using a 10-fold cross-validation procedure (kfold).\n",
    "\n",
    "Evaluating the model returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = cross_val_score(cmodel, X, yhot, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (result.mean()*100, result.std()*100))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.7 Compile and evaluate model on test data  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = baseline_model()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, yhot_train, validation_split=0.33,\n",
    "                    epochs=200, batch_size=100, verbose=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.8 Plot the learning curve  \n",
    "\n",
    "The plots are provided below. The history for the validation dataset is labeled test by convention as it is indeed a test dataset for the model.\n",
    "\n",
    "From the plot of accuracy we can see that the model could probably be trained a little more as the trend for accuracy on both datasets is still rising for the last few epochs. We can also see that the model has not yet over-learned the training dataset, showing comparable skill on both datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test, yhot_test)\n",
    "print('Accuracy from evaluate: %.2f' % (accuracy*100))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predict_x = model.predict(X_test)\n",
    "pred = np.argmax(predict_x, axis=1)\n",
    "print(f'Prediction Accuracy: {(pred == y_test).mean() * 100:f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.9 Save the model  \n",
    "\n",
    "The model is then converted to JSON format and written to model.json in the local directory. The network weights are written to model.h5 in the local directory.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"models/customermodel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Reload models from disk and predict  \n",
    "\n",
    "## 2.1 Look at our files\n",
    "\n",
    "The model and weight data is loaded from the saved files and a new model is created. It is important to compile the loaded model before it is used. This is so that predictions made using the model can use the appropriate efficient computation from the Keras backend.\n",
    "\n",
    "The model is evaluated in the same way printing the same evaluation score.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ls -l models"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Reload the model  \n",
    "\n",
    "We will reload our data, simulating the vent where we maay be wanting to run a prediction a day or two later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('modelcustomer.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Reload data  \n",
    "\n",
    "Reload our training data, but take a 10% random sample  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/customertrain.csv')\n",
    "df = df.sample(frac=0.10)\n",
    "df.dropna(inplace=True)\n",
    "df = df.drop([\"Segmentation\",\"ID\"], axis=1) # These fields not features we can use\n",
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, when we realod Y, we first want to load our original encoder. Naturally, we cannot have new categories, else we will get an error at this point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepareYreload(df):\n",
    "    # yencoder = LabelEncoder()\n",
    "    # yencoder.classes_ = np.load('models/y_classes.npy', allow_pickle=True) # reload saved class from training\n",
    "    yencoder = load(\"models/yencoder.joblib\")\n",
    "    return yencoder.transform(df[\"Var_1\"])\n",
    "\n",
    "y = prepareYreload(df)\n",
    "df = df.drop([\"Var_1\"], axis=1)\n",
    "pd.DataFrame(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepareFeaturesReload(df):\n",
    "    \"\"\"Prepares feature for predictions, this time, use classes saved for labelencoder and scaler\n",
    "\n",
    "      Parameters:\n",
    "      df (DataFrame): The dataframe with features\n",
    "\n",
    "      Returns:\n",
    "      X (nparray): a normalised array of all features\n",
    "    \"\"\"  \n",
    "    # Encode string features to numerics\n",
    "    columns = df.select_dtypes(include=['object']).columns\n",
    "    print(columns)\n",
    "    for feature in columns:\n",
    "      print('models/'+feature+'.joblib')\n",
    "      fencoder = load('models/'+feature+'.joblib')\n",
    "      print(fencoder.classes_)\n",
    "      df[feature] = fencoder.fit(df[feature])\n",
    "        # le = LabelEncoder()\n",
    "        # le.classes_ = np.load('models/'+feature+'_classes.npy', allow_pickle=True) # reload saved class from training\n",
    "        # df[feature] = le.fit(df[feature])\n",
    "\n",
    "    # fill in missing features with mean values\n",
    "    features_missing = df.columns[df.isna().any()]\n",
    "    for feature in features_missing:\n",
    "      fillmissing(df, feature= feature, method= \"mean\")    \n",
    "\n",
    "X, df = prepareFeaturesReload(df)\n",
    "pd.DataFrame(X).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predict_x = loaded_model.predict(X)\n",
    "pred = np.argmax(predict_x, axis=1)\n",
    "print(f'Prediction Accuracy: {(pred == Y).mean() * 100:f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Conclusion\n",
    "\n",
    "In this post you discovered how to develop and evaluate a neural network using the Keras Python library for deep learning.\n",
    "You learned:  \n",
    "  \n",
    "- How to load data and make it available to Keras.  \n",
    "- How to prepare multi-class classification data for modeling using one hot encoding.  \n",
    "- How to use Keras neural network models with scikit-learn.  \n",
    "- How to define a neural network using Keras for multi-class classification.  \n",
    "- How to evaluate a Keras neural network model using scikit-learn with k-fold cross validation  \n",
    "\n",
    "Some interesting things to observe:  \n",
    "\n",
    "With batch size of 5, we end up with 66.10% accuracy:  \n",
    "\n",
    "- Without normalising, it takes 3200 seconds for cross_val_score  \n",
    "- With normalising, it takes 1422 seconds for cross_val_score  \n",
    "\n",
    "With batch size of 100, we end up with an accuracy of 66.38%:  \n",
    "\n",
    "- Without normalising, it takes 83 seconds for cross_val_score  \n",
    "- With normalising, it takes 78 seconds for cross_val_score  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df2 = pd.read_csv('data/customertrain.csv')\n",
    "df2.drop([\"Segmentation\",\"ID\",\"Var_1\"], axis=1, inplace=True)\n",
    "df2 = df2.dropna()\n",
    "df2.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Gender Ever_Married  Age Graduated  Profession  Work_Experience  \\\n",
       "0    Male           No   22        No  Healthcare              1.0   \n",
       "2  Female          Yes   67       Yes    Engineer              1.0   \n",
       "3    Male          Yes   67       Yes      Lawyer              0.0   \n",
       "5    Male          Yes   56        No      Artist              0.0   \n",
       "6    Male           No   32       Yes  Healthcare              1.0   \n",
       "\n",
       "  Spending_Score  Family_Size  \n",
       "0            Low          4.0  \n",
       "2            Low          1.0  \n",
       "3           High          2.0  \n",
       "5        Average          2.0  \n",
       "6            Low          3.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ever_Married</th>\n",
       "      <th>Age</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Spending_Score</th>\n",
       "      <th>Family_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>22</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>56</td>\n",
       "      <td>No</td>\n",
       "      <td>Artist</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Average</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>32</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer = SimpleImputer(strategy='most_frequent')\n",
    "# X = pd.DataFrame(imputer.fit_transform(df2))\n",
    "# X.columns = df2.columns\n",
    "# X.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "numerical_ix = df2.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_ix = df2.select_dtypes(include=['object', 'bool']).columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "column_trans = ColumnTransformer(\n",
    "     [('cat', OneHotEncoder(),categorical_ix),\n",
    "      ('num', MinMaxScaler(feature_range=(-1, 1)), numerical_ix)],\n",
    "     remainder='drop')\n",
    "\n",
    "column_trans.fit(df2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
       "                                 Index(['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score'], dtype='object')),\n",
       "                                ('num', MinMaxScaler(feature_range=(-1, 1)),\n",
       "                                 Index(['Age', 'Work_Experience', 'Family_Size'], dtype='object'))])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "X = column_trans.transform(df2)\n",
    "pd.DataFrame(X).head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   11   12   13   14  \\\n",
       "0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "1  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  0.0  1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0  0.0   \n",
       "3  0.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "\n",
       "    15   16   17        18        19    20  \n",
       "0  0.0  0.0  1.0 -0.887324 -0.857143 -0.25  \n",
       "1  0.0  0.0  1.0  0.380282 -0.857143 -1.00  \n",
       "2  0.0  1.0  0.0  0.380282 -1.000000 -0.75  \n",
       "3  1.0  0.0  0.0  0.070423 -1.000000 -0.75  \n",
       "4  0.0  0.0  1.0 -0.605634 -0.857143 -0.50  \n",
       "\n",
       "[5 rows x 21 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.887324</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.380282</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380282</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.605634</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "column_trans.get_feature_names"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method ColumnTransformer.get_feature_names of ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
       "                                 Index(['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score'], dtype='object')),\n",
       "                                ('num', MinMaxScaler(feature_range=(-1, 1)),\n",
       "                                 Index(['Age', 'Work_Experience', 'Family_Size'], dtype='object'))])>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')   \n",
    "# diplays HTML representation in a jupyter context\n",
    "column_trans  "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
       "                                 Index(['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score'], dtype='object')),\n",
       "                                ('num', MinMaxScaler(feature_range=(-1, 1)),\n",
       "                                 Index(['Age', 'Work_Experience', 'Family_Size'], dtype='object'))])"
      ],
      "text/html": [
       "<style>#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 {color: black;background-color: white;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 pre{padding: 0;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-toggleable {background-color: white;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-item {z-index: 1;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-parallel-item:only-child::after {width: 0;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-472f3a4b-6c79-43eb-b825-27b685b4c8d6\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9158b4ff-67ae-4de5-be2a-4c6fe2fbc258\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9158b4ff-67ae-4de5-be2a-4c6fe2fbc258\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('cat', OneHotEncoder(),\n",
       "                                 Index(['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score'], dtype='object')),\n",
       "                                ('num', MinMaxScaler(feature_range=(-1, 1)),\n",
       "                                 Index(['Age', 'Work_Experience', 'Family_Size'], dtype='object'))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2c293102-c4c9-4dc0-a604-db9b03785b0f\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2c293102-c4c9-4dc0-a604-db9b03785b0f\">cat</label><div class=\"sk-toggleable__content\"><pre>Index(['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score'], dtype='object')</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"baa35bd3-b987-4815-b91e-0f4c8112a762\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"baa35bd3-b987-4815-b91e-0f4c8112a762\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6c891649-a02b-4a7e-af07-e3e326d49a23\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6c891649-a02b-4a7e-af07-e3e326d49a23\">num</label><div class=\"sk-toggleable__content\"><pre>Index(['Age', 'Work_Experience', 'Family_Size'], dtype='object')</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"538ee161-7268-4590-846c-4ecf40bff1d1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"538ee161-7268-4590-846c-4ecf40bff1d1\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler(feature_range=(-1, 1))</pre></div></div></div></div></div></div></div></div></div></div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}