{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    "To begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.read_csv('customertest.csv')\n",
    "df1.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('customertrain.csv')\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare the data  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns = [\"Gender\",\"Ever_Married\",\"Graduated\",\"Profession\",\"Spending_Score\"]\n",
    "for feature in columns:\n",
    "    le = LabelEncoder()\n",
    "    df[feature] = le.fit_transform(df[feature])\n",
    "\n",
    "df = df.drop([\"Segmentation\"], axis=1)        \n",
    "df = df.drop([\"ID\"], axis=1)        \n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove rows with no data  \n",
    "\n",
    "We dont want to work with rows that do not have a Var_1 value, so lets remove them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dropna(subset=['Var_1'], inplace=True)\n",
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encode Y\n",
    "Use fit_transform to encode our multinomial categories into numbers we can work with. Later on, we can change back to labels with  \n",
    "```\n",
    "df[\"Var_1\"] = yle.inverse_transform(df[\"Var_1\"])\n",
    "```\n",
    "\n",
    "We can also print the list of categories as follows:  \n",
    "```\n",
    "print(yle.classes_)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yle = LabelEncoder()\n",
    "df[\"Var_1\"] = yle.fit_transform(df[\"Var_1\"])\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fill in missing features  \n",
    "\n",
    "An important part of regression is understanding which features are missing. We can choose to ignore all rows with missing values, or fill them in with either mode, median or mode.\n",
    "\n",
    "- Mode = most common value\n",
    "- Median = middle value\n",
    "- Mean = average\n",
    "\n",
    "Here is a handy function you can call which will fill in the missing features by your desired method. We will choose to fill in values with the average.  \n",
    "After funning below, you should see 7992 with no null values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fillmissing(df, feature, method):\n",
    "    if method == \"mode\":\n",
    "        df[feature] = df[feature].fillna(df[feature].mode()[0])\n",
    "        \n",
    "    elif method == \"median\":\n",
    "        df[feature] = df[feature].fillna(df[feature].median())\n",
    "        \n",
    "    else:\n",
    "        df[feature] = df[feature].fillna(df[feature].mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_missing= df.columns[df.isna().any()]\n",
    "for feature in features_missing:\n",
    "    fillmissing(df, feature= feature, method= \"mean\")\n",
    "df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract Y column from the dataframe  \n",
    "\n",
    "Lets extract our Y column into a seperate array and remove it from the dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Y = df[\"Var_1\"]\n",
    "df = df.drop([\"Var_1\"], axis=1) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot correlation matrix\n",
    "\n",
    "If we have a data set with many columns, a good way to quickly check correlations among columns is by visualizing the correlation matrix as a heatmap. Looking at the matrix, you can see 9 columns that have the highest correlation above 0.38. We are only interested in `life expectancy`, so look at the bottom row for your results.   "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize = (24,16))\n",
    "sns.heatmap(pd.concat([df,Y], axis=1).corr(), annot=True, cmap=\"coolwarm\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not great features here, but lets see how we go...  \n",
    "The only ones that look reasonable are Gender, Ever_Married, Agem Graduated and maybe Work_Experience. You usually prefer these to be above 0.3."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get X/Y into arrays\n",
    "\n",
    "Now copy out our X and y columns into matrix's for easier matrix manipulation later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df.to_numpy()  # np.matrix(df.to_numpy())\n",
    "y = Y.to_numpy().transpose()  # np.matrix(Y.to_numpy()).transpose()\n",
    "\n",
    "m,n = X.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalise X  \n",
    "\n",
    "Now, lets normalise X so the values lie between -1 and 1. We do this so we can get all features into a similar range. We use the following equation  \n",
    "$X_{(i)} = \\frac{x_{(i)}-mean(x)}{max(x)-min(x)}$  \n",
    "  \n",
    "The goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values. This process of rescaling the features is so that they have mean as 0 and variance as 1.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mu = X.mean(0) # \n",
    "sigma = X.std(0) # standard deviation: max(x)-min(x)\n",
    "xn = (X - mu) / sigma"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add a column of ones to X for easier matrix manipulation of our hypothesis and cost function later"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xo = np.hstack((np.ones((m, 1)), xn))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup our neural network\n",
    "\n",
    "Then, we can setup the sizes of our neural network, first, below is the neural network we want to put together.  \n",
    "![title](neuralnetwork.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below initialisations, ensure above network is achieved. So, now you are asking \"What are reasonable numbers to set these to?\"  \n",
    "  \n",
    "`Input layer`   = set to the size of the dimensions  \n",
    "`Hidden layers` = set to input_layer * 2  \n",
    "`Output layer`  = set to the size of the labels of Y. In our case, this is 7 categories  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_layer_size = n                    # Dimension of features\n",
    "hidden_layer_size = input_layer_size*2  # of units in hidden layer \n",
    "output_layer_size = len(yle.classes_)     # number of labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialise weights  \n",
    "\n",
    "Now, we can initialise our weights to random small values (remember these are also called thetaâ€™s). For gradient descent, its OK to initalise to zero's, but for neural networks, it works out better if we initialise our weights to some random values. Here we develop a handy function to perform the initialization.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def initializeWeights(L_in, L_out):\n",
    "    epsilon_init = 0.12\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "    return W"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "initial_Theta1 = initializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = initializeWeights(hidden_layer_size, output_layer_size)\n",
    "nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()), axis=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sigmoid functions  \n",
    "\n",
    "Since we are doing classification, we will use sigmoid to evaluate our predictions. A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula  \n",
    "\n",
    "$$sigmoid(z) = g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "$$g'(z) = \\frac{d}{dz}g(z) = g(z)\\left(1 - g(z)\\right)$$\n",
    "\n",
    "In checknn.py the following handy functions are created:  \n",
    "  \n",
    "- sigmoid is a handy function to compute sigmoid of input parameter Z\n",
    "- sigmoidGradient computes the gradient of the sigmoid function evaluated at z. This should work regardless if z is a matrix or a vector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization  \n",
    "\n",
    "We will implement regularization as one of the most common problems data science professionals face is to avoid overfitting. Overfitting gives you a situation where your model performed exceptionally well on train data but was not able to predict test data. Neural network are complex and makes them more prone to overfitting. Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the modelâ€™s performance on the unseen data as well.  \n",
    "\n",
    "If you have studied the concept of regularization in machine learning, you will have a fair idea that regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.  \n",
    "We implement regularization in nnCostFunction by passing in a lambda which us used to penalise both the gradients and costs that are calculated. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cost function  \n",
    "\n",
    "We need a function which can implements the neural network cost function for a two layer neural network which performs classification.  \n",
    "In checknn.py out costfunction will return  \n",
    "\n",
    "- gradient should be a \"unrolled\" vector of the partial derivatives of the neural network  \n",
    "- the final J which is the cost of this weight.   \n",
    "\n",
    "Our cost function will do the following:  \n",
    "\n",
    "- Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices for our 2 layer neural network\n",
    "- Perform forward propagation to calculate (a) and (z)\n",
    "$${\\left(\\Theta_{ji}^{(l)}\\right)^2} = -\\frac{1}{m}trace\\left(y^T\\log\\left(h_\\Theta(X)\\right) + ({\\bf1} - y)^T\\log\\left({\\bf1} - h_\\Theta(X)\\right)\\right) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}{\\left(\\Theta_{ji}^{(l)}\\right)^2}$$ \n",
    "- Calculate the cost of our forward propagation into J and apply regularization \n",
    "$$J(\\theta) = -\\frac{1}{m}\\left[\\sum_{i=1}^{m}\\sum_{k=1}^{K}{y_k^{(i)}\\log\\left(h_\\Theta(x^{(i)})_k\\right) + (1 - y_k^{(i)})\\log\\left(1 - h_\\Theta(x^{(i)})_k\\right)}\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}$$  \n",
    "- Perform backward propagation to calculate (s) and apply regularization \n",
    "$$\\Delta_{ij}^{(l)} = \\sum_m{a_j^{(l)}\\delta_i^{(l+1)}}$$\n",
    "$$D^{(l)} = \\frac{1}{m}\\Delta^{(l)} + \\frac{\\lambda}{m}\\Theta^{(l)}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking our forward/backward propagation  \n",
    "\n",
    "One difficult thing to understand is if our cost function is performing well. A good method to check this is to run a function called checknn.  \n",
    "Creates a small neural network to check the backpropagation gradients, it will output the analytical gradients produced by your backprop code and the numerical gradients (computed using computeNumericalGradient). These two gradient computations should result in very similar values.  \n",
    "\n",
    "If you want to delve more into the theory behind this technique, it is tought in Andrew Ng's machine learning course, week 4.  \n",
    "You do not need to run this every time, just when you have setup your network.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from checknn import *\n",
    "\n",
    "print('Checking Backpropagation... ')\n",
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lambda_ = 1\n",
    "#  Check gradients by running checkNNGradients\n",
    "checkNNGradients(lambda_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from checknn import *\n",
    "\n",
    "print('Checking Cost Function (w/ Regularization) ... ')\n",
    "J, g = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, output_layer_size, xn, y, lambda_)\n",
    "\n",
    "print(f'Cost at parameters (loaded from ex4weights): {J:f} \\n(this value should be about 0.383770)')\n",
    "g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Training Neural Network... ')\n",
    "\n",
    "#  Change the MaxIter to a larger value to see how more training helps.\n",
    "options = {'maxiter': 50, 'disp': True}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1;\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "fun = lambda nn_params: nnCostFunction(nn_params, input_layer_size, hidden_layer_size, output_layer_size, xn, y, lambda_)[0]\n",
    "jac = lambda nn_params: nnCostFunction(nn_params, input_layer_size, hidden_layer_size, output_layer_size, xn, y, lambda_)[1]\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument (the neural network parameters)\n",
    "from scipy import optimize as opt\n",
    "res = opt.minimize(fun, nn_params, method='CG', jac=jac, options=options)\n",
    "nn_params = res.x\n",
    "cost = res.fun\n",
    "\n",
    "print(res.message)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = nn_params[:hidden_layer_size * (input_layer_size + 1)].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].reshape((output_layer_size, hidden_layer_size + 1))\n",
    "\n",
    "print(cost)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = predict(Theta1, Theta2, X)\n",
    "\n",
    "print(f'Training Set Accuracy: {(pred == y).mean() * 100:f}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}